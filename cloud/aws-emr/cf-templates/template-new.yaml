---
AWSTemplateFormatVersion: '2010-09-09'
Description: Cloudformation Template to spin up Variant Spark on EMR clusters V3 (Version 5 of EMR only)
Parameters:
  # Random-Forest Parameters Group
  NumTree:
    Description: (-rn) Number of trees to built.
    Type: Number
    Default: "100"
  mtry:
    Description: (-rmt) Number of Randomly selected variable to test at each node of each tree.
    Type: Number
    Default: "1000"
  mtryFraction:
    Description: (-rmtf) mtry as a fraction of number of variable in the input file.
    Type: Number
    Default: "0.1"
  #VariantSpark Parameters Group
  rfBatchSize:
    Description: (-rbs) Number of trees to be built in parallel
    Type: Number
    Default: "100"
  PhenotypeFile:
    Description: (-ff) S3 path to phenotype file. It should be in CSV format. The first row in header, the first column is be sample name.
    Type: String
    Default: "s3://csiro-tb/share/Hipster2/Hipster.csv"
  PhenotypeColumn:
    Description: (-fc) The column name in the phenotype file that hold the phenotype value. Phenotype is binary and can be 0 (a control) or 1 (a case).
    Type: String
    Default: "Hipster"
  InputFile:
    Description: (-if) S3 path to the input file. Can be in CSV or VCF format. See InputType parameter for more details. 
    Type: String
    Default: "s3://csiro-tb/share/Hipster2/VCF/Small.558938.vcf.bz2"
  InputType:
    Description: (-it) It can be VCF or CSV. The VCF file can be compressed using bzip2 (x.vcf.bz2) or bgzip (x.vcf.bgz). For the CSV file the first row is sample name and the first column is the variable name. All Variable should be ordinal, descreet and starting from 0 like 0,1,2,3,...,n where n is a small number.
    Type: String
    Default: "VCF"
    AllowedValues:
      - vcf
      - csv
  MaxValue:
    Description: (-ivo) If the input is VCF, VariantSpark translate 0/0, 0/1 and 1/1 genotypes to 0, 1 and 2 respectively and treat them as descreet ordinal values. Variant spark ignore phasing information so 0/1, 1/0, 0|1 and 1|0 genotypes are all translated to 1. VariantSpark also ignore multiallelic genotype so 0/1, 0/3 and 2/3 genotypes are translated to 1, 1 and 2 respectively. So each variable can take only 3 different values (0,1,2). In case the input file is CSV, there could be a variable that takes more than 3 values. This parameter should be set to the maximum number of values a variable can take. For example if it sets to 6 (0,1,2,3,4,5) none of the variable can have value greater than 5.
    Type: Number
    Default: "3"
  OutputFile:
    Description: (-of) S3 path to the output file (importance score)
    Type: String
    #should not have a default value
    Default: "s3://csiro-tb/share/Hipster2/test-maciej1.out"
  ModelFile:
    Description: (-om) S3 path to the file. The program saves produced Random-Forest model in this file. For the file format see ModelType parameter
    Type: String
    #should not have a default value
    Default: "s3://csiro-tb/share/Hipster2/test-maciej1.out"
  ModelType:
    Description: (??) The constructed Random-Forest model can be stored in java and json format. The json output is not optimised and may take a long time to produce. the java model file is only provided for programatic access.
    Type: String
    Default: "java"
    AllowedValues:
      - java
      - json
  Biallelic:
    Description: (-ivb) If input is VCF and multiallelic site are broken into biallelic site (two or more variants have the same chr-pos) you should set this option to True. If set to the true the output file also include alternate allele for each variants. By default the output file only include chr-pos for each variant.
    Type: String
    Default: "False"
    AllowedValues:
      - "Flase"
      - "True"
  OOB:
    Description: (-ro) Compute out-of-bag error rate for each batch of tree and the whole forest. If set to true slowdown the process. 
    Type: String
    Default: "False"
    AllowedValues:
      - "Flase"
      - "True"
  Seed:
    Description: (-sr) Random seed to be able to replicate the same process. 
    Type: Number
    Default: "13"
  # Hardware Group
    MasterNodePricing:
      Description: Do you want master node EC2 instance with spot pricing or on-demand pricing.
      Type: String
      Default: "Spot"
      AllowedValues:
        - Spot
        - OnDemand
    SpotNumCPU:
      Description: Number of spot CPU for core instances. It is not the number of instances.
      Type: Number
      Default: "32"
    OnDemandNumCPU:
      Description: Number of OnDemand CPU for core instances. It is not the number of instances.
      Type: Number
      Default: "0"
  #General EMR options
  clusterName:
    Description: Name of the cluster
    Type: String
  taskInstanceCount:
    Description: Number of task instances
    Type: String
  emrVersion:
    Description: Version of EMR
    Type: String
    Default: "emr-5.12.0"
    ConstraintDescription: 'Must be EMR Version 5 (i.e: emr-5.3.0)'
  masterInstanceType:
    Description: Instance type of Master Node
    Type: String
    Default: "m4.large"
    AllowedValues: 
      - m4.large
  coreInstanceType:
    Description: Instance type of Core Node
    Type: String
    Default: "m4.large"
    AllowedValues:
      - m4.large
  taskInstanceType:
    Description: Instance type of Task Node
    Type: String
    Default: "m4.large"
    AllowedValues: 
      - m4.large
  environmentType:
    Description: What environment do you want the cluster to be in
    Type: String
    Default: "live"
  s3BucketBasePath:
    Description: Bucket to log EMR actions to
    Type: String
  taskBidPrice:
    Description: Bid price for Task nodes
    Type: String
    Default: "OnDemandPrice"
  terminationProtected:
    Description: Is the cluster to have termination protection enabled
    Type: String
    AllowedValues:
    - 'true'
    - 'false'
    Default: 'false'
    ConstraintDescription: Boolean
  awsRegion:
    Description: awsRegion
    Default: ap-southeast-2
    AllowedValues:
    - ap-southeast-2
    Type: String
Conditions:
  isLive:
    Fn::Equals:
    - Ref: environmentType
    - live
Resources:
  EMRClusterV5:
    Type: AWS::EMR::Cluster
    Properties:
      Instances:
        MasterInstanceGroup:
          InstanceCount: 1
          InstanceType:
            Ref: masterInstanceType
          Market: ON_DEMAND
          Name: Master instance group - 1
        CoreInstanceGroup:
          InstanceCount: 1
          InstanceType:
            Ref: coreInstanceType
          Market: ON_DEMAND
          Name: Core instance group - 2
        TerminationProtected:
          Ref: terminationProtected
      BootstrapActions:
      - Name: Install Jupyter
        ScriptBootstrapAction:
          Path: "s3://csiro-tb/lib/jupyter/bootstrap/install-jupyter.sh"
      - Name: Install Hail
        ScriptBootstrapAction:
          Args:
            - "--input-path"
            - "s3://csiro-tb/lib/hail/0.1_2.2.1"
            - "--hail-version"
            - "0.1"
            - "--spark-version"
            - "2.2.1"
          Path: "s3://csiro-tb/lib/jupyter/bootstrap/install-hail.sh"
      Configurations:
      - Classification: emrfs-site
        ConfigurationProperties:
          fs.s3.maxConnections: 500
      - Classification: spark
        ConfigurationProperties:
          maximizeResourceAllocation: 'true'
      - Classification: spark-defaults
        ConfigurationProperties:
          spark.sql.files.openCostInBytes: '1099511627776'
          spark.hadoop.io.compression.codecs: 'org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,org.apache.hadoop.io.compress.GzipCodec'
          spark.hadoop.parquet.block.size: '1099511627776'
          spark.executor.extraClassPath: '/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/home/hadoop/hail-all-spark.jar'
          spark.locality.wait: '10s'
          spark.driver.extraClassPath: '/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/home/hadoop/hail-all-spark.jar'
          spark.serializer: 'org.apache.spark.serializer.KryoSerializer'
          spark.sql.files.maxPartitionBytes: '1099511627776'
          spark.dynamicAllocation.enabled: 'false'
      Applications:
      - Name: Ganglia
      - Name: Spark
      Name:
        Ref: clusterName
      JobFlowRole: "EMR_EC2_DefaultRole"
      ServiceRole: "EMR_DefaultRole"
      ReleaseLabel:
        Ref: emrVersion
      LogUri:
        Ref: s3BucketBasePath
      VisibleToAllUsers: true
      Steps:
      - Name: "VariantSpark Job"
        ActionOnFailure: "CANCEL_AND_WAIT"
        HadoopJarStep: 
          Args:
          - "spark-submit"
          - "--deploy-mode"
          - "client"
          - "--class"
          - "au.csiro.variantspark.cli.VariantSparkApp"
          - "/home/hadoop/miniconda2/envs/jupyter/lib/python2.7/site-packages/varspark/jars/variant-spark_2.11-0.2.0-a1-all.jar"
          - "importance"
          - "-if"
          - Ref: InputFile
          - "-it"
          - Ref: InputType
          - "-of"
          - Ref: OutputFile
          - "-ff"
          - Ref: PhenotypeFile
          - "-fc"
          - Ref: PhenotypeColumn
          - "-om"
          - Ref: ModelFile
          - "-rn"
          - Ref: NumTree
          - "-rmt"
          - Ref: mtry
          - "-rmtf"
          - Ref: mtryFraction
          - "-rbs"
          - Ref: rfBatchSize
          - "-ivo"
          - Ref: MaxValue
          - "-sr"
          - Ref: Seed
          - "-ro"
          - Ref: rfBatchSize
          - "-on"
          - "100000000"
          Jar: "command-runner.jar"
      Tags:
      - Key: Name
        Value:
          Fn::Join:
          - ''
          - - emr-instance-
            - Ref: AWS::StackName
            - ''
      - Key: Environment
        Value:
          Ref: environmentType
      - Key: Stack ID
        Value:
          Ref: AWS::StackName
  EMRTaskNodes:
    Type: AWS::EMR::InstanceGroupConfig
    Properties:
      InstanceCount:
        Ref: taskInstanceCount
      InstanceType:
        Ref: taskInstanceType
      BidPrice:
        Ref: taskBidPrice
      Market: SPOT
      InstanceRole: TASK
      Name: Task instance group - 3
      JobFlowId:
        Ref: EMRClusterV5
Outputs:
  ClusterID:
    Description: "EMR Cluster ID"
    Value: !Ref EMRClusterV5
